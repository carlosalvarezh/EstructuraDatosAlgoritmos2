{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec7257b",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Estructura de Datos y Algoritmos II</h1>\n",
    "<h1 align=\"center\">Algoritmos Avanzados</h1>\n",
    "<h1 align=\"center\">Aprendizaje Automático - Regresión y Clasificación</h1>\n",
    "<h1 align=\"center\">2024</h1>\n",
    "<h1 align=\"center\">MEDELLÍN - COLOMBIA </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f12a6f",
   "metadata": {},
   "source": [
    "*** \n",
    "|[![Outlook](https://img.shields.io/badge/Microsoft_Outlook-0078D4?style=plastic&logo=microsoft-outlook&logoColor=white)](mailto:calvar52@eafit.edu.co)||[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/S08_AlgoritmosAvanzados_ML_ClasificacionRegresion.ipynb)\n",
    "|-:|:-|--:|\n",
    "|[![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?style=plastic&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/carlosalvarez5/)|[![@alvarezhenao](https://img.shields.io/twitter/url/https/twitter.com/alvarezhenao.svg?style=social&label=Follow%20%40alvarezhenao)](https://twitter.com/alvarezhenao)|[![@carlosalvarezh](https://img.shields.io/badge/github-%23121011.svg?style=plastic&logo=github&logoColor=white)](https://github.com/carlosalvarezh)|\n",
    "\n",
    "<table>\n",
    " <tr align=left><td><img align=left src=\"https://github.com/carlosalvarezh/Curso_CEC_EAFIT/blob/main/images/CCLogoColorPop1.gif?raw=true\" width=\"25\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license.(c) Carlos Alberto Alvarez Henao</td>\n",
    "</table>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a25268-465b-46cc-9855-1992737ba325",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751d41d-aa9e-4a9a-9e98-004197a4b567",
   "metadata": {},
   "source": [
    "Recordemos, del capítulo anterior, los tipos de Aprendizaje Automático que se tienen:\n",
    "\n",
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/ML04.png?raw=true\" width=\"500\" />\n",
    "</p>\n",
    "\n",
    "- ***Aprendizaje Supervisado:*** Involucra modelos que aprenden a partir de un conjunto de datos etiquetados. El objetivo es aprender una función que mapee las entradas a salidas deseadas. Ejemplos incluyen clasificación (e.g., distinguir entre correos electrónicos spam y no spam) y regresión (e.g., predecir precios de viviendas).\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Aprendizaje No Supervisado:*** Se usa con datos que no están etiquetados. El sistema intenta aprender la estructura de los datos sin ninguna etiqueta, a menudo a través de la identificación de patrones. Ejemplos comunes son el clustering (e.g., segmentación de clientes) y la reducción de dimensionalidad.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Aprendizaje Semi-supervisado:*** Combina ambos métodos anteriores. Utiliza una pequeña cantidad de datos etiquetados y una gran cantidad de datos no etiquetados. Es útil cuando las etiquetas son costosas de obtener.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Aprendizaje por Refuerzo:*** En este tipo, los algoritmos aprenden a tomar decisiones realizando acciones en un entorno para maximizar alguna noción de recompensa acumulativa. Ejemplos incluyen juegos (e.g., ajedrez, Go) y navegación de robots.\n",
    "\n",
    "En este capítulo nos centraremos exclusivamente en revisar algunos de los algoritmos más ampliamente empleados en el Aprendizaje Supervisado, que incluye algoritmos de Clasificación y de Regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26b03c",
   "metadata": {},
   "source": [
    "## Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dca57",
   "metadata": {},
   "source": [
    " ### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d0a45",
   "metadata": {},
   "source": [
    "La [regresión](https://en.wikipedia.org/wiki/Regression_analysis) en el aprendizaje automático es una técnica esencial que permite a los científicos de datos predecir y entender las relaciones complejas entre variables. Al centrarse en predecir valores continuos, se diferencia de los modelos de [clasificación](https://en.wikipedia.org/wiki/Statistical_classification), que predicen categorías discretas. Los tipos más comunes de regresión incluyen la [regresión lineal](https://en.wikipedia.org/wiki/Linear_regression), tanto simple como múltiple, que se basan en el ajuste de una línea o plano a los datos para predecir la variable dependiente a partir de una o más variables independientes.\n",
    "\n",
    "Aparte de la regresión lineal, existen técnicas avanzadas como la [regresión polinómica](https://en.wikipedia.org/wiki/Polynomial_regression), que modela relaciones no lineales, y métodos de regularización como la regresión de cresta y [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)), que son efectivos para manejar la multicolinealidad y evitar el sobreajuste. Además, existen enfoques no paramétricos, como los [árboles de decisión](https://en.wikipedia.org/wiki/Decision_tree) y los [Random forest](https://en.wikipedia.org/wiki/Random_forest), que ofrecen mayor flexibilidad en la modelización de relaciones complejas. La elección del modelo adecuado depende de varios factores, incluyendo la naturaleza de los datos y el problema específico a resolver. La evaluación del modelo, un paso crucial en cualquier análisis de regresión, se realiza a través de métricas como el error cuadrático medio y el coeficiente de determinación.\n",
    "\n",
    "El núcleo de cualquier problema de regresión es la relación entre:\n",
    "\n",
    "1. **Variable Independiente (Predictora):** Estas son las variables de entrada que se utilizan para hacer las predicciones.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "2. **Variable Dependiente (Objetivo):** Es el resultado que intentamos predecir o estimar.\n",
    "\n",
    "En la regresión, buscamos una función que mapee de la mejor manera las variables independientes con la dependiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59639c4",
   "metadata": {},
   "source": [
    "### Acercamiento al problema de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c143f3",
   "metadata": {},
   "source": [
    "Supongamos que estamos trabajando con tres puntos específicos: $[1,2]$, $[2,3]$, y $[3,3.5]$. ¿Podríamos identificar una recta que se ajuste de la mejor manera posible a estos puntos? En otras palabras, ¿existe una recta única que pase exactamente por todos ellos? Y si no, ¿hay alguna recta que se destaque como la \"mejor\" entre todas las posibles?\n",
    "\n",
    "Si estos puntos estuvieran perfectamente alineados, una única recta los intersectaría a todos. Sin embargo, una observación detallada revela que no están colineales. Esto implica que, aunque para cada par de puntos podemos trazar una recta específica, resultando en tres rectas diferentes para nuestros tres puntos, ninguna de ellas será la solución perfecta. Este escenario se evidencia claramente en la representación gráfica de estas rectas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646c447",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data points\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([2, 3, 3.5])\n",
    "\n",
    "# Ecuaciones de las rectas\n",
    "def recta_roja(x):\n",
    "    return x + 1\n",
    "\n",
    "def recta_verde(x):\n",
    "    return 0.5 * x + 2\n",
    "\n",
    "# Extender el rango de X para las líneas roja y verde\n",
    "x_roja_extendida = np.linspace(1, 3, 100)\n",
    "x_verde_extendida = np.linspace(1, 3, 100)\n",
    "\n",
    "# Calcular Y para las líneas extendidas\n",
    "y_roja_extendida = recta_roja(x_roja_extendida)\n",
    "y_verde_extendida = recta_verde(x_verde_extendida)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x, y, color='blue', label='Data Points')\n",
    "\n",
    "# Dibujar y extender las líneas\n",
    "plt.plot(x_roja_extendida, y_roja_extendida, color='red', label='Line 1')\n",
    "plt.plot(x_verde_extendida, y_verde_extendida, color='green', label='Line 2')\n",
    "plt.plot([x[0], x[2]], [y[0], y[2]], color='orange', label='Line 3')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9adc65",
   "metadata": {},
   "source": [
    "Cada recta que consideramos posee una pendiente ($m$) y un punto de intersección con el eje $y$ ($b$) únicos. Al explorar las diversas combinaciones posibles de $m$ y $b$, buscamos de manera visual e intuitiva aquella recta que mejor se ajuste a nuestros puntos, es decir, la que minimice la suma de las distancias perpendiculares entre la recta y cada uno de los puntos. Para facilitar este proceso, hemos desarrollado un [widget](https://ipywidgets.readthedocs.io/en/7.x/) interactivo. Este permite ajustar los valores de $m$ y $b$ y, simultáneamente, observar el impacto de estos ajustes en la alineación de la recta con respecto a los puntos de datos. No solo muestra las distancias individuales de cada punto a la recta, sino también la suma total de estas distancias, proporcionando una comprensión más clara de la calidad del ajuste de la recta a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5390c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "\n",
    "# Data points\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([2, 3, 3.5])\n",
    "\n",
    "# Función para calcular la distancia de un punto a una recta\n",
    "def distance_from_line(point_x, point_y, m, b):\n",
    "    return abs(m * point_x - point_y + b) / np.sqrt(m**2 + 1)\n",
    "\n",
    "# Función para dibujar la recta, los puntos y calcular la suma de distancias\n",
    "def plot_line_and_distances(m, b):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Dibujar los puntos originales\n",
    "    plt.scatter(x, y, color='blue', label='Data Points')\n",
    "    \n",
    "    # Crear una línea con la pendiente y la intersección dados\n",
    "    x_vals = np.linspace(0, 4, 100)\n",
    "    y_vals = m * x_vals + b\n",
    "    plt.plot(x_vals, y_vals, color='red', label=f'y = {m}x + {b}')\n",
    "\n",
    "    # Calcular y mostrar las distancias\n",
    "    total_distance = 0\n",
    "    for i, (xi, yi) in enumerate(zip(x, y)):\n",
    "        dist = distance_from_line(xi, yi, m, b)\n",
    "        total_distance += dist\n",
    "        plt.plot([xi, xi], [yi, m*xi+b], color='green', linestyle='--')\n",
    "        plt.text(xi, (yi + m*xi+b)/2, f'{dist:.2f}', color='green')\n",
    "\n",
    "    # Mostrar la suma total de las distancias\n",
    "    plt.title(f'Suma total de distancias: {total_distance:.2f}')\n",
    "\n",
    "    # Configuración del gráfico\n",
    "    plt.xlim(0, 4)\n",
    "    plt.ylim(0, 5)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Crear el widget interactivo\n",
    "interactive_plot = interactive(plot_line_and_distances, m=(-2.0, 2.0, 0.01), b=(0, 5, 0.01))\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d0364",
   "metadata": {},
   "source": [
    "Realizar este proceso de forma \"manual\" resulta no solo tedioso sino también impreciso. Para abordar esta tarea de manera más eficiente y exacta, recurriremos a una técnica matemática conocida como *Regresión Lineal Simple*, también denominada [*OLS*](https://en.wikipedia.org/wiki/Ordinary_least_squares) por sus siglas en inglés (*Ordinary Least Squares*). Esta metodología se basa en encontrar la línea que minimiza la suma de los cuadrados de las diferencias (*errores*) entre los valores observados y los valores predichos por la recta, proporcionando así un ajuste óptimo y matemáticamente fundamentado a los puntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ed856",
   "metadata": {},
   "source": [
    "### Regresión Lineal Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b03b1",
   "metadata": {},
   "source": [
    "La [regresión lineal](https://en.wikipedia.org/wiki/Linear_regression) es una técnica estadística que busca modelar la relación entre una variable dependiente (también conocida como variable de respuesta) y una o más variables independientes (también conocidas como predictores o covariables) a través de una ecuación lineal. La forma más simple de regresión lineal se llama *regresión lineal simple*, donde hay una sola variable independiente. Cuando hay más de una variable independiente, se habla de *regresión lineal múltiple*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11018ef3",
   "metadata": {},
   "source": [
    "#### Aspectos matemáticos de la Regresión Lineal Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048198b",
   "metadata": {},
   "source": [
    "La ecuación básica para un modelo de regresión lineal simple es:\n",
    "\n",
    "$$y=\\beta_0+\\beta_1 \\cdot x + \\varepsilon$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $y$ es la variable dependiente a predecir.\n",
    "- $x$ son las variables predictoras.\n",
    "- $\\beta_0$ es la intersección (ordenada al origen) con el eje $y$\n",
    "- $\\beta_1$ es la pendiente de la línea de regresión.\n",
    "- $\\varepsilon$ es el término de error\n",
    "\n",
    "El objetivo principal es encontrar los valores de $\\beta_0$ y $\\beta_1$ que minimicen el error total (sumatoria de los errores al cuadrado) entre los valores reales de $y$ y los valores predichos por el modelo.\n",
    "\n",
    "***Nota:*** Cabe destacar que la ecuación utilizada aquí es idéntica a la ecuación clásica de una línea recta, expresada como $y = b + mx$. Esta forma se aplica típicamente cuando solo contamos con dos puntos, a través de los cuales pasa una única recta. Sin embargo, la ecuación del modelo de regresión lineal cobra especial relevancia cuando manejamos más de dos puntos. En este contexto, la regresión no busca una línea que pase exactamente por todos los puntos, sino una que minimice la distancia global entre la línea y cada uno de los puntos, ofreciendo así una representación más precisa y útil en situaciones donde los datos no siguen una línea perfecta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d0018",
   "metadata": {},
   "source": [
    "#### Minimización del error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b8ebe",
   "metadata": {},
   "source": [
    "Para lograr esto, utilizamos el método de [mínimos cuadrados](https://en.wikipedia.org/wiki/Least_squares). Este método busca encontrar los valores de  $\\beta_0$ y $\\beta_1$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados (\"reales\") y los valores predichos. Matemáticamente, esto se representa como:\n",
    "\n",
    "\n",
    "$$\\text{Minimizar} \\sum_{i=1}^n\\left(y_i - \\beta_1 x_i -\\beta_0\\right)^2$$\n",
    "\n",
    "donde $n$ es el número de observaciones. Esta suma de cuadrados es a menudo llamada \"*suma de los residuales al cuadrado*\".\n",
    "\n",
    "Cuando ajustamos los valores de $\\beta_0$ y $\\beta_1$, estamos esencialmente moviendo y rotando la línea de regresión en el gráfico para encontrar la posición que minimiza el error. Si ajustamos estos parámetros de manera correcta, la línea de regresión pasará lo más cerca posible de los puntos reales, y el error (la distancia entre los puntos y la línea) será mínima (como fue realizado de forma manual arriba)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16413861",
   "metadata": {},
   "source": [
    "#### Cálculo de los coeficientes estadísticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e1563",
   "metadata": {},
   "source": [
    "Para minimizar la función se toman las derivadas parciales con respecto a $\\beta_0$ y $\\beta_1$, se igualan a cero y se resuelven para ambas ecuaciones, dando los valores de los coeficientes:\n",
    "\n",
    "- ***Coeficientes de Regresión:***\n",
    "\n",
    "Los coeficientes de regresión $\\beta_0$ y $\\beta_1$ se calculan mediante las siguientes fórmulas:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\beta_1=\\frac{\\sum_{i-1}^n\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)}{\\sum_{i-1}^n\\left(x_i-\\bar{x}\\right)^2} \\\\\n",
    "\\\\\n",
    "& \\beta_0=\\bar{y}-\\beta_1 \\cdot \\bar{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Donde:\n",
    "- $n$ es el número de observaciones.\n",
    "- $x_i$ y $y_i$ son los valores de la variable independiente e dependiente respectivamente en la observación $i$.\n",
    "- $\\bar{x}$ y $\\bar{y}$ son las medias de las variables $x$ e $y$ respectivamente.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Coeficiente de Determinación $\\left(R^2\\right)$:***\n",
    "\n",
    "El coeficiente de determinación $R^2$ mide la proporción de la variabilidad total en la variable dependiente que es explicada por la regresión. Se calcula así:\n",
    "\n",
    "$$R^2=\\frac{S S R}{S S T}$$\n",
    "\n",
    "- $S S R$ es la suma de los cuadrados de la regresión.\n",
    "- $S S T$ es la suma total de los cuadrados.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Error Estándar de Estimación (SEE):***\n",
    "\n",
    "EI SEE es una medida de cuánto difieren los valores reales de la variable dependiente de los valores predichos por el modelo. Se calcula como:\n",
    "\n",
    "$$S E E=\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i-\\bar{y}_i\\right)^2}{n-2}}$$\n",
    "\n",
    "Donde:\n",
    "- $y_i$ son los valores reales de la variable dependiente.\n",
    "- $\\bar{y}_i$ son los valores predichos por el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1986c",
   "metadata": {},
   "source": [
    "#### Indicadores de bondad de ajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a08aa",
   "metadata": {},
   "source": [
    "Para evaluar la calidad del modelo de regresión lineal, utilizamos varios indicadores:\n",
    "\n",
    "- ***Coeficiente de Determinación ($R^2$):*** Mide la proporción de la variabilidad de la variable dependiente que es explicada por el modelo. $R^2$ varía entre $0$ y $1$, siendo $1$ indicativo de una perfecta predicción.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Error Estándar Residual (RSE):*** Estima la dispersión de los residuos (diferencias entre los valores reales y los predichos).\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***$p$-valor:*** Determina la significancia estadística de los coeficientes. Valores pequeños indican que la variable es relevante en el modelo.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Intervalos de Confianza para los Coeficientes:*** Ayudan a establecer el rango probable de valores para los coeficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4368c57",
   "metadata": {},
   "source": [
    "#### Implementación en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454171f4",
   "metadata": {},
   "source": [
    "Vamos a explorar la implementación de un modelo de regresión lineal utilizando Python y su ecosistema de bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bba9eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# datos del ejemplo\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([2, 3, 3.5])\n",
    "\n",
    "beta1 = np.sum((x - np.mean(x)) * (y - np.mean(y))) / np.sum((x - np.mean(x))**2)\n",
    "beta0 = np.mean(y) - beta1 * np.mean(x)\n",
    "\n",
    "y_pred = beta0 + beta1 * x\n",
    "\n",
    "R2 = np.sum((y_pred - np.mean(y))**2) / np.sum((y - np.mean(y))**2)\n",
    "SEE = np.sqrt(np.sum((y - y_pred)**2) / (len(x) - 2))\n",
    "\n",
    "# Graficar los datos y la recta de ajuste\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(x, y, label='Datos')\n",
    "plt.plot(x, beta1*x + beta0, 'r', label='Ajuste Lineal')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.title('Ajuste Lineal de Datos Aleatorios')\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Pendiente (m):\", beta1)\n",
    "print(\"Intercepto (c):\", beta0)\n",
    "print(\"RMSE:\", SEE)\n",
    "print(\"Coeficiente de Determinación (R^2):\", R2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec00f0f",
   "metadata": {},
   "source": [
    "Obsérvese que se obtuvieron resultados similares, sin realizar el procedimiento ensayo-error, y con una formulación matemática más robusta.\n",
    "\n",
    "Ahora, avancemos con otro ejemplo, esta vez utilizando un conjunto de datos más amplio. Para ello, generaremos valores de manera aleatoria, también conocidos como \"[datos sintéticos](https://en.wikipedia.org/wiki/Synthetic_data)\", con el objetivo de contar con una gran cantidad de datos que sigan una tendencia lineal. En nuestro próximo ejemplo, utilizaremos una ecuación de recta predefinida, a la cual añadiremos cierto grado de \"ruido\" aleatorio. Este ruido asegura que los datos no sean colineales perfectamente y que exista un margen de error, imitando de manera más realista las condiciones que a menudo encontramos en datos del mundo real. Este enfoque nos permite observar cómo las técnicas de regresión lineal se aplican y se ajustan en situaciones con variabilidad y ruido en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6725c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generación de datos aleatorios\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, 20)\n",
    "y = 2 * x + 1 + np.random.normal(0, 1, 20)\n",
    "\n",
    "beta1 = np.sum((x - np.mean(x)) * (y - np.mean(y))) / np.sum((x - np.mean(x))**2)\n",
    "beta0 = np.mean(y) - beta1 * np.mean(x)\n",
    "\n",
    "y_pred = beta0 + beta1 * x\n",
    "\n",
    "R2 = np.sum((y_pred - np.mean(y))**2) / np.sum((y - np.mean(y))**2)\n",
    "SEE = np.sqrt(np.sum((y - y_pred)**2) / (len(x) - 2))\n",
    "\n",
    "\"\"\"\n",
    "# Cálculo de los parámetros del ajuste lineal\n",
    "# empleando conceptos de Algebra Lineal para su solución\n",
    "A = np.vstack([x, np.ones(len(x))]).T\n",
    "m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "# Estadísticas del ajuste\n",
    "residuals = y - (m*x + c)\n",
    "rmse = np.sqrt(np.mean(residuals**2))\n",
    "r_squared = 1 - (np.sum(residuals**2) / np.sum((y - np.mean(y))**2))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Graficar los datos y la recta de ajuste\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(x, y, label='Datos')\n",
    "plt.plot(x, beta1*x + beta0, 'r', label='Ajuste Lineal')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.title('Ajuste Lineal de Datos Aleatorios')\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Pendiente (m):\", beta1)\n",
    "print(\"Intercepto (c):\", beta0)\n",
    "print(\"RMSE:\", SEE)\n",
    "print(\"Coeficiente de Determinación (R^2):\", R2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777a5f7",
   "metadata": {},
   "source": [
    "#### Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1816d",
   "metadata": {},
   "source": [
    "Tanto el Coeficiente de Determinación $R^2$, como el *Root Mean Square Error* (*RMSE*) varían entre $0$ y $1$ en su escala.\n",
    "\n",
    "- $R^2$ varía entre $0$ y $1$. Un valor de $0$ significa que el modelo no explica nada de la variabilidad en los datos, mientras que un valor de $1$ significa que el modelo explica toda la variabilidad en los datos.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- El *RMSE* es siempre un número positivo y varía en magnitud. No se mide en una escala de $0$ a $1$ como el $R^2$. Cuanto más pequeño sea el valor del *RMSE*, más cercano será el ajuste del modelo a los datos reales. Un *RMSE* de $0$ indicaría que el modelo encaja perfectamente con los datos observados, lo cual es poco probable en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c5e7f",
   "metadata": {},
   "source": [
    "#### Ejemplo: Cuarteto de Anscombe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a3e77",
   "metadata": {},
   "source": [
    "El [Cuarteto de Anscombe](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) es un conjunto de cuatro conjuntos de datos que tienen propiedades estadísticas muy similares, pero visualmente distintas. Fue presentado por el estadístico *Francis Anscombe* en 1973 como una poderosa demostración de la importancia de visualizar los datos antes de realizar análisis y tomar decisiones basadas en ellos. Cada conjunto de datos tiene 11 observaciones (puntos), lo que lo convierte en un ejemplo convincente para ilustrar cómo los resultados pueden ser engañosos si solo se confía en estadísticas descriptivas.\n",
    "\n",
    "|  x1   |  y1   |$|$|  x2   |  y2   |$|$| x3   |  y3   |$|$| x4   |  y4   |\n",
    "|-------|-------|-  |-------|-------|-  |------|-------|-  |------|-------|\n",
    "| 10.0  |  8.04 |$|$| 10.0  |  9.14 |$|$|10.0  |  7.46 |$|$| 8.0  |  6.58 |\n",
    "|  8.0  |  6.95 |$|$|  8.0  |  8.14 |$|$| 8.0  |  6.77 |$|$| 8.0  |  5.76 |\n",
    "| 13.0  |  7.58 |$|$| 13.0  |  8.74 |$|$|13.0  | 12.74 |$|$| 8.0  |  7.71 |\n",
    "|  9.0  |  8.81 |$|$|  9.0  |  8.77 |$|$| 9.0  |  7.11 |$|$| 8.0  |  8.84 |\n",
    "| 11.0  |  8.33 |$|$| 11.0  |  9.26 |$|$|11.0  |  7.81 |$|$| 8.0  |  8.47 |\n",
    "| 14.0  |  9.96 |$|$| 14.0  |  8.10 |$|$|14.0  |  8.84 |$|$| 8.0  |  7.04 |\n",
    "|  6.0  |  7.24 |$|$|  6.0  |  6.13 |$|$| 6.0  |  6.08 |$|$| 8.0  |  5.25 |\n",
    "|  4.0  |  4.26 |$|$|  4.0  |  3.10 |$|$| 4.0  |  5.39 |$|$|19.0  | 12.50 |\n",
    "| 12.0  | 10.84 |$|$| 12.0  |  9.13 |$|$|12.0  |  8.15 |$|$| 8.0  |  5.56 |\n",
    "|  7.0  |  4.82 |$|$|  7.0  |  7.26 |$|$| 7.0  |  6.42 |$|$| 8.0  |  7.91 |\n",
    "|  5.0  |  5.68 |$|$|  5.0  |  4.74 |$|$| 5.0  |  5.73 |$|$| 8.0  |  6.89 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4453a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cuarteto de Anscombe\n",
    "anscombe_1 = np.array([\n",
    "    [10.0, 8.04], [8.0, 6.95], [13.0, 7.58],\n",
    "    [9.0, 8.81], [11.0, 8.33], [14.0, 9.96],\n",
    "    [6.0, 7.24], [4.0, 4.26], [12.0, 10.84],\n",
    "    [7.0, 4.82], [5.0, 5.68]\n",
    "])\n",
    "\n",
    "anscombe_2 = np.array([\n",
    "    [10.0, 9.14], [8.0, 8.14], [13.0, 8.74],\n",
    "    [9.0, 8.77], [11.0, 9.26], [14.0, 8.10],\n",
    "    [6.0, 6.13], [4.0, 3.10], [12.0, 9.13],\n",
    "    [7.0, 7.26], [5.0, 4.74]\n",
    "])\n",
    "\n",
    "anscombe_3 = np.array([\n",
    "    [10.0, 7.46], [8.0, 6.77], [13.0, 12.74],\n",
    "    [9.0, 7.11], [11.0, 7.81], [14.0, 8.84],\n",
    "    [6.0, 6.08], [4.0, 5.39], [12.0, 8.15],\n",
    "    [7.0, 6.42], [5.0, 5.73]\n",
    "])\n",
    "\n",
    "anscombe_4 = np.array([\n",
    "    [8.0, 6.58], [8.0, 5.76], [8.0, 7.71],\n",
    "    [8.0, 8.84], [8.0, 8.47], [8.0, 7.04],\n",
    "    [8.0, 5.25], [19.0, 12.50], [8.0, 5.56],\n",
    "    [8.0, 7.91], [8.0, 6.89]\n",
    "])\n",
    "\n",
    "anscombe_data = [anscombe_1, anscombe_2, anscombe_3, anscombe_4]\n",
    "labels = [\"Cuarteto 1\", \"Cuarteto 2\", \"Cuarteto 3\", \"Cuarteto 4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef58ea",
   "metadata": {},
   "source": [
    "Calculando los estadísticos de cada conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc11477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticos del conjuntos de datos\n",
    "for i, data in enumerate(anscombe_data):\n",
    "    x, y = data[:, 0], data[:, 1]\n",
    "       \n",
    "    # Calcular estadísticos\n",
    "    mean_x, mean_y = np.mean(x), np.mean(y)\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    correlation = np.corrcoef(x, y)[0, 1]\n",
    "    \n",
    "    print(f\"\\nEstadísticas para {labels[i]}:\")\n",
    "    print(f\"Media de X: {mean_x:.2f}\")\n",
    "    print(f\"Media de Y: {mean_y:.2f}\")\n",
    "    print(f\"Pendiente: {slope:.2f}\")\n",
    "    print(f\"Intercepto: {intercept:.2f}\")\n",
    "    print(f\"Coeficiente de Correlación: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c92e4e",
   "metadata": {},
   "source": [
    "¿Qué podemos decir de los resultados de los estadísticos calculados? Tómese su tiempo para pensar y discutirlo, basado en la teoría.\n",
    "\n",
    "Ahora grafiquemos los datos de donde se obtuvieron dichos resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los conjuntos de datos y calcular estadísticos\n",
    "for i, data in enumerate(anscombe_data):\n",
    "    x, y = data[:, 0], data[:, 1]\n",
    "    \n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.scatter(x, y, label=labels[i])\n",
    "    plt.plot(x, np.polyval(np.polyfit(x, y, 1), x), 'r', label='Línea de Tendencia')\n",
    "    plt.title(labels[i])\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b81c02",
   "metadata": {},
   "source": [
    "### Ecuación de Regresión Lineal Múltiple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c8d07",
   "metadata": {},
   "source": [
    "#### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d638d0c",
   "metadata": {},
   "source": [
    "La Regresión Lineal Múltiple es una técnica fundamental en el campo de la estadística y el análisis de datos. Se trata de una extensión poderosa de la Regresión Lineal Simple, diseñada para abordar situaciones en las que una variable dependiente $Y$ se relaciona con dos o más variables independientes $X_1,X_2,\\ldots,X_n$. La principal pregunta que busca responder es cómo estas variables independientes influyen en la variable dependiente y cómo pueden ser utilizadas para predecir su comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56648c88",
   "metadata": {},
   "source": [
    "#### Aspectos matemáticos de la ecuación de regresión lineal múltiple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08293574",
   "metadata": {},
   "source": [
    "La ecuación general de la Regresión Lineal Múltiple es:\n",
    "\n",
    "$$Y=\\beta_0+\\beta_1 X_1+\\beta_2 X_2+\\ldots+\\beta_n X_n+\\varepsilon$$\n",
    "\n",
    "Aquí:\n",
    "\n",
    "- $Y$ es la variable dependiente que estamos tratando de predecir o entender.\n",
    "- $X_1, X_2, \\ldots, X_p$ son las variables independientes o predictores.\n",
    "- $\\beta_0$ es el término de intersección o el valor esperado de $Y$ cuando todas las $X$ son cero.\n",
    "- $\\beta_1, \\beta_2, \\ldots, \\beta_n$ son los coeficientes de regresión, que indican cómo cambia $Y$ en respuesta a cambios en las $X$.\n",
    "- $\\varepsilon$ representa el error, que captura la variabilidad no explicada por el modelo.\n",
    "\n",
    "A medida que se van adicionando más variables, de una $x$ a dos ($x_1$ y $x_2$), ya el problema cambia de *\"encontrar la recta que mejor se ajusta a una serie de datos\"* (en *2D*), a *encontrar el mejor plano que se ajuste a la nube de puntos tridimensionales* (3D). Y a medida que se añaden más variables, ya estaríamos hablando de hiperplanos en espacios multidimensionales. Cada una de las dimensiones representando una característica de la realidad del fenómeno que los datos representan. Se puede tener una gran cantidad de atributos y, por lo tanto, de dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280df022",
   "metadata": {},
   "source": [
    "#### Forma vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4536d",
   "metadata": {},
   "source": [
    "La forma más práctica de representar esta combinación lineal de variables para cada uno de nuestros datos es empleando la nomenclatura vectorial, es decir, en vez de tener un conjunto de ecuaciones del tipo:\n",
    "\n",
    "$$y_1=\\beta_0+\\beta_1x_{11}+\\beta_2x_{12}+\\beta_3x_{13}+\\ldots$$\n",
    "$$y_2=\\beta_0+\\beta_1x_{21}+\\beta_2x_{22}+\\beta_3x_{23}+\\ldots$$\n",
    "$$y_3=\\beta_0+\\beta_1x_{31}+\\beta_2x_{32}+\\beta_3x_{33}+\\ldots$$\n",
    "$$\\vdots$$\n",
    "\n",
    "se puede conformar una matriz en la que cada columna representa una característica de los datos de entrada, y las filas representa cada una de las mediciones que se tienen en el conjunto de datos\n",
    "\n",
    "$$\\textbf{X}=\\begin{bmatrix} 1 & x_{11} & x_{12} & x_{13} & \\ldots \\\\ 1 & x_{21} & x_{22} & x_{23} & \\ldots \\\\ 1 & x_{31} & x_{32} & x_{33} & \\ldots \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots \\end{bmatrix}$$\n",
    "\n",
    "Para cada una de las ecuaciones se tiene la variable $y$, que es la variable a modelar. Juntando todas en un vector, se tiene\n",
    "\n",
    "$$\\textbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3  \\\\ \\vdots\\end{bmatrix}$$\n",
    "\n",
    "y se puede hacer lo mismo con los factores para generar un vector de parámetros\n",
    "\n",
    "$$\\textbf{B}=\\begin{bmatrix} \\beta_0 & \\beta_1 & \\beta_2 & \\beta_3 \\ldots\\end{bmatrix}$$\n",
    "\n",
    "Con esto, se puede reducir la escitura de una gran cantidad de ecuaciones a una sola ecuación vectorial escrita como\n",
    "\n",
    "$$\\textbf{Y}=\\textbf{X}\\textbf{B}$$\n",
    "\n",
    "Esta forma de escribir no es solo estética, sino que nos facilitará la comprensión del algoritmo a programar además de ser mucho más eficiente, ya que los procesadores y GPUs procesan de forma más eficiente sobre este tipo de estructuras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53e9fe",
   "metadata": {},
   "source": [
    "#### Ecuación Matricial y Mínimos Cuadrados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4ac55",
   "metadata": {},
   "source": [
    "La ecuación matricial que resume este proceso es:\n",
    "\n",
    "$$\\textbf{B}=\\left(\\textbf{X}^T \\textbf{X}\\right)^{-1} \\textbf{X}^T Y$$\n",
    "\n",
    "- $\\textbf{X}$ es la matriz de diseño que contiene los valores de las variables independientes para cada observación.\n",
    "- $\\textbf{Y}$ es el vector de valores de la variable dependiente.\n",
    "- $\\textbf{X}^T$ es la matriz transpuesta de $X$.\n",
    "- $\\left(\\textbf{X}^T \\textbf{X}\\right)^{-1}$ es la inversa de la matriz producto $\\textbf{X}^T \\textbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588bdde",
   "metadata": {},
   "source": [
    "#### Evaluación de la calidad del ajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d45df",
   "metadata": {},
   "source": [
    "Una vez que se obtienen los coeficientes $\\beta$, es crucial evaluar la calidad del ajuste del modelo. Algunos estadísticos clave son:\n",
    "\n",
    "- ***Coeficiente de Determinación $\\left(R^2\\right)$ :*** Mide la proporción de la variabilidad total en \n",
    "$\\textbf{Y}$ explicada por el modelo. Varía entre $0$ (ninguna explicación) y $1$ (explicación completa).\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Error Estándar Residual (RMSE o $\\sigma$ ):*** Representa la desviación estándar de los residuos. Un RMSE más bajo indica un mejor ajuste.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Estadístico F y Prueba t:*** Evalúan la significancia del modelo y de $\\sigma_z$ coeficientes individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c205ea",
   "metadata": {},
   "source": [
    "#### Ejemplo: Casas de Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f38e8",
   "metadata": {},
   "source": [
    "##### Descripción del Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135ca34",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizaremos el conjunto de datos de las casas de Boston, que contiene información sobre diferentes atributos de viviendas en Boston y sus respectivos valores medios. El objetivo es predecir el valor medio de las viviendas ($\\textbf{Y}$) utilizando cuatro variables independientes: tasa de criminalidad ($\\texttt{CRIM}$), cantidad promedio de habitaciones por vivienda ($\\texttt{RM}$), proporción de viviendas ocupadas por sus propietarios construidas antes de 1940 ($\\texttt{AGE}$) y tasa de impuesto a la propiedad ($\\texttt{TAX}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7200c0",
   "metadata": {},
   "source": [
    "##### Ruta de Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e1397",
   "metadata": {},
   "source": [
    "- ***Carga de Datos:*** Se carga el conjunto de datos de las casas de Boston utilizando la función `load_boston` de [`sklearn.datasets`](https://scikit-learn.org/stable/datasets.html). Luego, se crea un DataFrame data para almacenar los atributos y el vector target para almacenar los valores medios de las viviendas.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Selección de Variables:*** Se eligen cuatro variables independientes específicas para este ejemplo: $\\texttt{CRIM}$, $\\texttt{RM}$, $\\texttt{AGE}$ y $\\texttt{TAX}$. Estas variables se almacenan en la matriz $\\textbf{X}$.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***División de Datos:*** Se divide el conjunto de datos en conjuntos de entrenamiento y prueba utilizando la función `train_test_split` de `sklearn.model_selection`. Esto nos permitirá evaluar el rendimiento del modelo en datos no vistos.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Creación y Ajuste del Modelo:*** Se crea un objeto de regresión lineal utilizando `LinearRegression` de `sklearn.linear_model`. Luego, se ajusta el modelo utilizando los datos de entrenamiento (`X_train` y `y_train`).\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Predicciones y Evaluación:*** Se realizan predicciones en el conjunto de prueba (`X_test`) utilizando el modelo ajustado. Se calculan los estadísticos de calidad del ajuste, como el *Error Estándar Residual (RMSE)* y el *Coeficiente de Determinación* ($R^2$), utilizando las funciones `mean_squared_error` y `r2_score` de `sklearn.metrics`.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Presentación de Resultados:*** Se presentan los coeficientes de ajuste, el término de intersección, el *RMSE* y el $R^2$ como resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952e2c8",
   "metadata": {},
   "source": [
    "##### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbb64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7798b7e",
   "metadata": {},
   "source": [
    "En vez de la biblioteca de sklearn usaremos la URL directa al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52312017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Cargar el conjunto de datos de las casas de Boston desde la URL\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eee631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con los datos y nombres de columnas\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "data_df = pd.DataFrame(data, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ffb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar cuatro variables independientes para el ejemplo\n",
    "selected_features = ['CRIM', 'RM', 'AGE', 'TAX']\n",
    "X = data_df[selected_features]\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y ajustar el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular estadísticos de calidad del ajuste\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Obtener los coeficientes de ajuste\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Presentar resultados\n",
    "print(\"Coeficientes de Ajuste:\", coefficients)\n",
    "print(\"Término de Intersección:\", intercept)\n",
    "print(\"Error Estándar Residual (RMSE):\", rmse)\n",
    "print(\"Coeficiente de Determinación (R^2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43a72e",
   "metadata": {},
   "source": [
    "##### Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd1735",
   "metadata": {},
   "source": [
    "Los coeficientes de ajuste representan cómo cada variable independiente contribuye al valor medio de las viviendas. En este caso:\n",
    "\n",
    "- El coeficiente de ajuste para $\\texttt{CRIM}$ es aproximadamente $-0.1354$. Esto sugiere que, manteniendo las otras variables constantes, un aumento en la tasa de criminalidad está asociado con una disminución de aproximadamente $\\$135$ por unidad en el valor medio de las viviendas.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- El coeficiente de ajuste para $\\texttt{RM}$ es aproximadamente $8.138$. Esto indica que, manteniendo las otras variables constantes, un aumento en la cantidad promedio de habitaciones por vivienda está relacionado con un aumento de aproximadamente $\\$8,138$ en el valor medio de las viviendas.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- El coeficiente de ajuste para $\\texttt{AGE}$ es aproximadamente $-0.0287$. Esto implica que un aumento en la proporción de viviendas ocupadas por sus propietarios construidas antes de 1940 está asociado con una disminución de aproximadamente $\\$28.7$ por unidad en el valor medio de las viviendas.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- El coeficiente de ajuste para $\\texttt{TAX}$ es aproximadamente $-0.0096$. Esto sugiere que un aumento en la tasa de impuesto a la propiedad está asociado con una disminución de aproximadamente $\\$9.6$ por unidad en el valor medio de las viviendas.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- El término de intersección (aproximadamente $-22.2662$) representa el valor esperado de las viviendas cuando todas las variables independientes son cero. Sin embargo, en este contexto, este valor puede no tener una interpretación directa y práctica debido a que muchas de las variables nunca serán iguales a cero en situaciones reales.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Error Estándar Residual (RMSE):*** El *RMSE* es una medida de la desviación promedio entre los valores reales y las predicciones del modelo. En este caso, el *RMSE* es aproximadamente 6.1324. Esto significa que, en promedio, las predicciones del modelo tienen un error de alrededor de $\\$6,132$ en relación con los valores reales de las viviendas. Un *RMSE* más bajo indica un mejor ajuste del modelo a los datos.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Coeficiente de Determinación ($R^2$):*** El $R^2$ de aproximadamente $0.4872$ indica que alrededor del $48.72\\%$ de la variabilidad en los valores medios de las viviendas puede ser explicada por el modelo de regresión lineal múltiple. Esto sugiere que el modelo tiene cierta capacidad para predecir las variaciones en los valores medios de las viviendas, pero no explica la variabilidad en su totalidad.\n",
    "\n",
    "***Análisis General***\n",
    "\n",
    "- Los resultados sugieren que el modelo de regresión lineal múltiple tiene cierta capacidad para predecir los valores medios de las viviendas en función de las variables seleccionadas ($\\texttt{CRIM}$, $\\texttt{RM}$, $\\texttt{AGE}$ y $\\texttt{TAX}$). Sin embargo, el $R^2$ relativamente bajo y el *RMSE* moderado indican que aún existe una cantidad significativa de variabilidad que el modelo no puede explicar. Esto podría deberse a la falta de consideración de otras variables importantes que influyen en el valor de las viviendas.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "En resumen, mientras que el modelo tiene utilidad predictiva, es importante considerar que hay otros factores que podrían influir en los valores de las viviendas y que no están considerados en este análisis. Sería recomendable explorar más variables y realizar análisis adicionales para mejorar la precisión y capacidad explicativa del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f77f2a",
   "metadata": {},
   "source": [
    "### Descenso del Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29676d7",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/Gradient03.gif?raw=true\" width=\"350\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a79dd",
   "metadata": {},
   "source": [
    "#### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2ed66",
   "metadata": {},
   "source": [
    "El Método del [Descenso del Gradiente](https://en.wikipedia.org/wiki/Gradient_descent)(*Gradient Descent*), aunque comúnmente considerado una técnica de optimización general, juega un papel crucial en el contexto de la regresión en el aprendizaje automático. Esta metodología se emplea para optimizar los modelos de regresión, ayudando a encontrar los parámetros del modelo que minimizan la función de costo, un paso esencial en la generación de predicciones precisas.\n",
    "\n",
    "La conexión entre el *Descenso del Gradiente* y la *Regresión* se puede resumir en:\n",
    "\n",
    "- **Optimización de Modelos de Regresión:** En la regresión, el objetivo es ajustar una línea (en regresión lineal) o una curva (en regresión no lineal) a los datos de manera que se minimice la diferencia entre los valores predichos y los reales. Aquí es donde el descenso del gradiente se vuelve crucial, ayudando a ajustar los parámetros del modelo (como los coeficientes en la regresión lineal) de manera eficiente.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Minimización de la Función de Costo:** La función de costo (como el error cuadrático medio en la regresión lineal) mide qué tan bien el modelo de regresión se ajusta a los datos. El descenso del gradiente busca minimizar esta función de costo ajustando iterativamente los parámetros del modelo.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Ajuste de Parámetros del Modelo:** En cada iteración, el descenso del gradiente ajusta los parámetros en dirección al gradiente negativo de la función de costo. Esto significa que los parámetros se modifican en la dirección que más reduce el error de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197b00d",
   "metadata": {},
   "source": [
    "#### ¿Qué es un gradiente?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237700dd",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/Gradient02.PNG?raw=true\" width=\"500\" />\n",
    "</p>\n",
    "\n",
    "Matemáticamente, el gradiente es un vector que representa la dirección y magnitud del mayor incremento de una función en un punto dado. En otras palabras, el gradiente apunta en la dirección de la mayor pendiente ascendente de la función.\n",
    "\n",
    "- **Definición del Gradiente:** Para una función $ f(x, y, z, \\dots) $ de varias variables, el gradiente de $ f $, denotado como $ \\nabla f $ o $ \\text{grad} \\, f $, es un vector que contiene todas las derivadas parciales de $ f $. Si $ f $ es una función de tres variables $ x $, $ y $, y $ z $, el gradiente de $ f $ es: \n",
    "     $$ \\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z} \\right) $$\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Interpretación Geométrica:** El gradiente en un punto particular ofrece la dirección en la que la función aumenta más rápidamente. La magnitud del gradiente en ese punto da la tasa de aumento más rápida.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **El Gradiente y el Descenso Máximo:** Aunque el gradiente apunta en la dirección de la mayor subida, en el método del descenso del gradiente lo utilizamos para movernos en la dirección opuesta. Esto se debe a que buscamos minimizar la función, no maximizarla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede47c84",
   "metadata": {},
   "source": [
    "#### El Gradiente en el Descenso del Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858eac9",
   "metadata": {},
   "source": [
    "El Descenso del Gradiente es una técnica de optimización para encontrar los parámetros de un modelo que minimizan la función de coste, esencial en el aprendizaje supervisado para la regresión.Matemáticamente, el gradiente de una función en un punto dado es un vector que apunta en la dirección de mayor aumento de la función. En el descenso del gradiente, tomamos pasos en la dirección opuesta al gradiente para reducir el valor de la función de coste.\n",
    "\n",
    "\n",
    "- **Cálculo del Gradiente en la Función de Costo:** Considerando una función de costo $ f(\\theta) $ que depende de los parámetros del modelo $ \\theta $, el gradiente de $ f $ con respecto a $ \\theta $, denotado como $\\nabla f(\\theta)$, indica en qué dirección cambiar $ \\theta $ para aumentar $ f $.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Uso del Gradiente para Actualizar Parámetros:** En cada iteración, los parámetros del modelo se actualizan en la dirección opuesta al gradiente. Esto se hace mediante la fórmula:\n",
    "\n",
    "     $$ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\nabla f(\\theta_{\\text{old}}) $$\n",
    "     \n",
    " Aquí, $ \\alpha $ es la tasa de aprendizaje, que controla el tamaño del paso en la actualización, es decir, $\\alpha$ determina el tamaño de los pasos que damos hacia el mínimo de la función de coste. Un $\\alpha$ pequeño conduce a una convergencia lenta, mientras que un $\\alpha$ grande podría saltar sobre el mínimo y nunca encontrarlo. Un valor óptimo de $\\alpha$ es crucial para asegurar que el algoritmo converge de manera eficiente sin perder precisión.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Convergencia a un Mínimo Local:** Repitiendo este proceso, ajustamos iterativamente los parámetros del modelo para moverse hacia un mínimo local en la función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f4777",
   "metadata": {},
   "source": [
    "**Ejemplo:** Regresión Lineal con Descenso del Gradiente en Python\n",
    "\n",
    "Sea la función:\n",
    "\n",
    "$$F(x,y)=\\sin \\left(\\frac{1}{2}x^2-\\frac{1}{4}y^2+3\\right)\\cos\\left(2x+1-e^y\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e248f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función objetivo para la optimización.\n",
    "# Esta función es específica y puede variar según el problema.\n",
    "func = lambda th: np.sin(1/2 * th[0] **2 - 1/4 * th[1] **2 + 3) * np.cos(2 * th[0] + 1 - np.e**th[1])\n",
    "\n",
    "# Configuración de la resolución de la malla para la visualización.\n",
    "res = 100\n",
    "X = np.linspace(-2, 2, res)\n",
    "Y = np.linspace(-2, 2, res)\n",
    "\n",
    "# Creación de una matriz de ceros para almacenar los valores de la función.\n",
    "Z = np.zeros((res, res))\n",
    "\n",
    "# Cálculo de los valores de la función en los puntos de la malla.\n",
    "for ix, x in enumerate(X):\n",
    "    for iy, y in enumerate(Y):\n",
    "        Z[iy, ix] = func([x, y])\n",
    "\n",
    "# Visualización de la función como un gráfico de contorno.\n",
    "plt.contourf(X, Y, Z, 100)\n",
    "plt.colorbar()\n",
    "\n",
    "# Generación de un punto aleatorio como posición inicial para el descenso del gradiente.\n",
    "Theta = np.random.rand(2) * 4 - 2\n",
    "\n",
    "# Copia de Theta para uso en el cálculo del gradiente.\n",
    "T = np.copy(Theta)\n",
    "\n",
    "# Parámetros para el cálculo del gradiente: tamaño del paso y tasa de aprendizaje.\n",
    "h = 0.001\n",
    "lr = 0.001\n",
    "\n",
    "# Representación gráfica del punto inicial.\n",
    "plt.plot(Theta[0], Theta[1], \"o\", c=\"white\")\n",
    "\n",
    "# Inicialización del vector gradiente.\n",
    "grad = np.zeros(2)\n",
    "\n",
    "# Proceso iterativo para realizar el descenso del gradiente.\n",
    "for j in range(10000):\n",
    "    # Cálculo de las derivadas parciales en cada dimensión.\n",
    "    for it, th in enumerate(Theta):\n",
    "        T = np.copy(Theta)\n",
    "        T[it] = T[it] + h\n",
    "        deriv = (func(T) - func(Theta)) / h\n",
    "        grad[it] = deriv\n",
    "\n",
    "    # Actualización de los parámetros Theta.\n",
    "    Theta = Theta - lr * grad\n",
    "    \n",
    "    # Representación gráfica de la trayectoria cada 100 iteraciones.\n",
    "    if (j % 100 == 0):\n",
    "        plt.plot(Theta[0], Theta[1], \".\", c=\"red\")\n",
    "\n",
    "# Representación gráfica de la posición final de Theta.\n",
    "plt.plot(Theta[0], Theta[1], \"o\", c=\"green\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12594453",
   "metadata": {},
   "source": [
    "Ahora apliquemos este método del descenso del gradiente al ejemplo inicial de ajustar la recta que pasa por los puntos $(1, 2)$, $(2, 3)$ y $(3, 3.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5533f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Puntos de datos\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([2, 3, 3.5])\n",
    "\n",
    "# Establecer límites fijos para el eje Y\n",
    "ylim_min = min(y) - 1\n",
    "ylim_max = max(y) + 1\n",
    "\n",
    "# Función de costo (error cuadrático medio)\n",
    "def cost_function(m, b, x, y):\n",
    "    total_error = 0.0\n",
    "    for i in range(len(x)):\n",
    "        total_error += (y[i] - (m * x[i] + b))**2\n",
    "    return total_error / len(x)\n",
    "\n",
    "# Parámetros iniciales más cercanos a 0\n",
    "m = np.random.rand() * 0.01\n",
    "b = np.random.rand() * 0.01\n",
    "\n",
    "# Tasa de aprendizaje y número de iteraciones\n",
    "lr = 0.01\n",
    "n_iterations = 3000\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    m_grad = 0\n",
    "    b_grad = 0\n",
    "    N = float(len(x))\n",
    "    for i in range(len(x)):\n",
    "        m_grad += -2/N * x[i] * (y[i] - (m * x[i] + b))\n",
    "        b_grad += -2/N * (y[i] - (m * x[i] + b))\n",
    "\n",
    "    m -= lr * m_grad\n",
    "    b -= lr * b_grad\n",
    "\n",
    "    if iteration % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.scatter(x, y, color='red')\n",
    "        y_pred = m * x + b\n",
    "        plt.plot(x, y_pred, color='blue')\n",
    "        plt.ylim(ylim_min, ylim_max)\n",
    "        error = cost_function(m, b, x, y)\n",
    "        plt.title(f\"Iteración: {iteration}, m = {m:.4f}, b = {b:.4f}, Error: {error:.8f}\")\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.show()\n",
    "\n",
    "# Mostrar los valores finales de m y b\n",
    "print(f\"Parámetros finales de la regresión: m = {m}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae3ef5",
   "metadata": {},
   "source": [
    "Como podemos observar, los valores obtenidos para $m$ y $b$ utilizando el método del Descenso del Gradiente difieren de aquellos hallados mediante el método de *Mínimos Cuadrados Ordinarios* (*OLS*). Para comprender por qué ocurre esto, es útil analizar el gráfico de la función de costo.\n",
    "\n",
    "En la regresión lineal, la función de costo suele formar una superficie convexa con un único mínimo global. No obstante, en ciertos casos, puede presentar múltiples mínimos locales, áreas planas o incluso puntos de silla de montar. El algoritmo de *Descenso del Gradiente*, dependiendo del punto de inicio (valores iniciales de $m$ y $b$), podría converger a diferentes mínimos locales. Esto resulta en variaciones en los valores finales de $m$ y $b$. Si experimentamos con distintos valores iniciales para $m$ y $b$, podremos observar cómo cambian los resultados.\n",
    "\n",
    "A pesar de estas diferencias, es importante notar que las líneas de regresión resultantes de ambos métodos, *Descenso del Gradiente* y *OLS*, suelen ser muy parecidas. Esto indica que, aunque los valores numéricos exactos de los parámetros pueden variar, la tendencia general que describen y su capacidad para modelar la relación entre las variables sigue siendo consistente y fiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the data\n",
    "X = np.array([1, 2, 3])  # Features\n",
    "y = np.array([2, 3, 3.5])  # Target\n",
    "\n",
    "# Define the cost function\n",
    "def cost_function(X, y, m, b):\n",
    "    n = len(X)\n",
    "    y_pred = m * X + b\n",
    "    return (1/(2*n)) * np.sum((y_pred - y)**2)\n",
    "\n",
    "# Generate values for m and b\n",
    "m_vals = np.linspace(-1, 2, 100)\n",
    "b_vals = np.linspace(-1, 2, 100)\n",
    "M, B = np.meshgrid(m_vals, b_vals)\n",
    "J_vals = np.array([cost_function(X, y, m, b) for m, b in zip(np.ravel(M), np.ravel(B))])\n",
    "J_vals = J_vals.reshape(M.shape)\n",
    "\n",
    "# Plot the cost function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(M, B, J_vals, cmap='viridis')\n",
    "ax.set_xlabel('m')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('Cost Function')\n",
    "ax.set_title('Cost Function J(m, b)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ccc413",
   "metadata": {},
   "source": [
    "#### Desafíos y Consideraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806729f0",
   "metadata": {},
   "source": [
    "- ***Elección de la Tasa de Aprendizaje:*** Una tasa de aprendizaje inadecuadamente elegida puede llevar a una convergencia muy lenta o a una divergencia.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Problemas de Convergencia:*** El descenso del gradiente puede quedar atrapado en mínimos locales o puntos de silla. Para resolver esto se pueden emplear técnicas avanzadas como momentum o adaptación de la tasa de aprendizaje.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Escalado de Características:*** El escalado adecuado de características puede acelerar significativamente la convergencia del algoritmo. La Normalización y estandarización son algunas técnicas comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3845c0c",
   "metadata": {},
   "source": [
    "### Otros modelos de Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f9360",
   "metadata": {},
   "source": [
    "En el vasto campo de los métodos de regresión, existen numerosas técnicas más allá de las básicas que no cubriremos en detalle en este curso. Entre estas, se destacan métodos avanzados y especializados como la [Regresión Ridge](https://en.wikipedia.org/wiki/Ridge_regression), que incorpora una penalización [L2](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) para manejar la multicolinealidad; [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)), conocida por su capacidad de selección de características a través de la penalización [L1](https://en.wikipedia.org/wiki/Taxicab_geometry); y [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization), que combina las estrategias de *Ridge* y *Lasso* para ofrecer un enfoque más equilibrado. Además, la [Regresión Polinomial](https://en.wikipedia.org/wiki/Polynomial_regression) nos permite modelar relaciones no lineales entre variables, mientras que los [Mínimos Cuadrados Parciales](https://en.wikipedia.org/wiki/Partial_least_squares_regression) se utilizan cuando se manejan datos con muchas variables correlacionadas. Por otro lado, la [Regresión de Soporte Vectorial](https://en.wikipedia.org/wiki/Support_vector_machine) (*SVR*), una adaptación de las *Máquinas de Soporte Vectorial*, es útil para modelos en espacios de alta dimensionalidad. Cada uno de estos métodos tiene aplicaciones específicas y ventajas en ciertos contextos, abriendo un abanico de posibilidades para el análisis de datos y la modelización predictiva más allá de las técnicas de regresión convencionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29329a4c-64e4-4447-bd38-24eb1364d958",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875775a",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/Classification05.jpg?raw=true\" width=\"250\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e29dd",
   "metadata": {},
   "source": [
    "### ¿Cómo funciona?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fde59f",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/Classification03.jpeg?raw=true\" width=\"250\" />\n",
    "</p>\n",
    "\n",
    "La clasificación nos ayuda a tomar decisiones, por ejemplo, a la hora de recoger tomates en un supermercado (\"verdes\", \"maduros\", \"podridos\"). En términos de aprendizaje automático, asignamos una etiqueta de una de las clases a cada tomate que tenemos en nuestras manos.\n",
    "\n",
    "La eficiencia en la recolección de tomates (algunos lo llamarían modelo de clasificación) depende de la precisión de sus resultados. Cuanto más a menudo vayas al supermercado (en lugar de enviar a tus padres o a tu pareja), mejor podrás elegir tomates frescos y deliciosos.\n",
    "\n",
    "Las máquinas son iguales. Para que un modelo de clasificación aprenda a predecir resultados con precisión, necesita muchos ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2db4c",
   "metadata": {},
   "source": [
    "### Tipos de algoritmos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3824f7",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/Classification04.PNG?raw=true\" width=\"750\" />\n",
    "</p>\n",
    "\n",
    "En el aprendizaje supervisado, la clasificación se refiere al proceso de predecir categorías discretas (etiquetas de clase) para instancias dadas. Existen varios tipos de clasificación, cada uno adecuado para diferentes tipos de problemas y conjuntos de datos. Los principales tipos de clasificación en aprendizaje supervisado son:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358074e",
   "metadata": {},
   "source": [
    "### Clasificación Binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08a87e",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/Classification03.jpeg?raw=true\" width=\"250\" />\n",
    "</p>\n",
    "\n",
    "La clasificación binaria significa que hay dos clases con las que trabajar que se relacionan entre sí como *verdaderas* o *falsas*. Imagina que tienes delante una enorme caja con tomates amarillos y rojos, pero para tu receta de pasta italiana solo necesitas los rojos. O en un diagnóstico médico, un clasificador binario para una enfermedad específica podría tomar los síntomas de un paciente como características de entrada y predecir si el paciente está sano o tiene la enfermedad. Los posibles resultados del diagnóstico son *positivos* y *negativos*. \n",
    "\n",
    "Veamos algunos de los algoritmos de clasificación binaria más usados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a052016",
   "metadata": {},
   "source": [
    "#### Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0486b",
   "metadata": {},
   "source": [
    "La [regresión logística](https://en.wikipedia.org/wiki/Logistic_regression) se utiliza para la clasificación binaria donde empleamos la [función sigmoide](https://en.wikipedia.org/wiki/Sigmoid_function), que toma como entrada las variables independientes y produce un valor de probabilidad entre $0$ y $1$.\n",
    "\n",
    "Por ejemplo, si tenemos dos clases, `Clase0` y `Clase1`, si el valor de la función logística para una entrada es mayor que $0.5$ (valor umbral), entonces pertenece a la `Clase1`, de lo contrario, pertenece a la `Clase0`. Se le llama regresión porque es una extensión de la regresión lineal, pero se utiliza principalmente para problemas de clasificación.\n",
    "\n",
    "***Función logística:***\n",
    "\n",
    "La *función logística*, también conocida como *función sigmoide*, es una función matemática que tiene una característica forma de *\"S\"*. Es ampliamente utilizada en el campo del aprendizaje automático, especialmente en la regresión logística y en redes neuronales, para modelar la probabilidad de una clase o evento.\n",
    "\n",
    "1. **Ecuación**: La función logística se define como:\n",
    "\n",
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/Logistic00.PNG?raw=true\" width=\"250\" />\n",
    "</p>\n",
    "cccccc\n",
    "\n",
    "   $$\\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "   Donde:\n",
    "   - $\\sigma(x)$ es el valor de salida de la función sigmoide.\n",
    "   - $x$ es el valor de entrada.\n",
    "   - $e$ es la base del logaritmo natural.\n",
    "\n",
    "2. **Características**:\n",
    "- **Rango de Salida**: La salida de la función sigmoide está siempre entre $0$ y $1$, lo que la hace útil para representar probabilidades. Recordemos que la probabilidad de una clase se puede medir como:   \n",
    "<p>&nbsp;</p>\n",
    "\n",
    "   $$P(y=1)=\\sigma(x)$$  \n",
    "   $$P(y=0)=1-\\sigma(x)$$    \n",
    "<p>&nbsp;</p>\n",
    "   \n",
    "- **Forma de S**: Tiene una región central donde ocurre una transición suave y pronunciada, que se aplana a medida que se aleja hacia cualquier extremo.  \n",
    "<p>&nbsp;</p>\n",
    "   \n",
    "- **Derivada**: La derivada de la función sigmoide es máxima en $x = 0$ y disminuye a medida que $x$ se aleja de $0$, lo cual es útil para el gradiente en algoritmos de aprendizaje.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "3. **Código en Python**\n",
    "\n",
    "    A continuación, se proporciona un código en Python que utiliza las bibliotecas [`matplotlib`](https://matplotlib.org/) y [`numpy`](https://numpy.org/) para visualizar la función sigmoide logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e5385e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definición de la función logística\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Generación de valores de x\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Calculando los valores y de la función logística\n",
    "y = logistic(x)\n",
    "\n",
    "# Creación del gráfico\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Función Logística\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"σ(x)\")\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.axhline(1, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db46dec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>Actividad para el estudiante:</strong>\n",
    "  <p>Investigue otras posibles funciones Sigmoides. Para cada una de ellas, indique su aplicación o ventaja/desventaja respecto a otras funciones.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d7df0",
   "metadata": {},
   "source": [
    "#### Supuestos de regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efcc96e",
   "metadata": {},
   "source": [
    "- ***Observaciones independientes:*** cada observación es independiente de la otra. lo que significa que no hay correlación entre ninguna variable de entrada.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Variables dependientes binarias:*** se asume que la variable dependiente debe ser binaria o dicotómica, lo que significa que solo puede tomar dos valores. Para más de dos categorías se utilizan funciones SoftMax.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Relación de linealidad entre variables independientes y probabilidades logarítmicas:*** la relación entre las variables independientes y las probabilidades logarítmicas de la variable dependiente debe ser lineal.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Sin valores atípicos (outliers):*** no debería haber valores atípicos en el conjunto de datos.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Tamaño de muestra grande:*** el tamaño de la muestra es suficientemente grande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f642512",
   "metadata": {},
   "source": [
    "##### ¿Cómo funciona la regresión logística?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e173f",
   "metadata": {},
   "source": [
    "El modelo de regresión logística transforma la salida de valor continuo de la función de [regresión lineal](https://en.wikipedia.org/wiki/Linear_regression) en una salida de valor categórico utilizando una función sigmoidea, que asigna cualquier conjunto de variables independientes de valor real ingresado a un valor entre $0$ y $1$. Esta función se conoce como función logística.\n",
    "\n",
    "Sean las características de entrada independientes:\n",
    "\n",
    "$$\n",
    "X=\\left[\\begin{array}{ccc}\n",
    "x_{11} & \\ldots & x_{1 m} \\\\\n",
    "x_{21} & \\ldots & x_{2 m} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n 1} & \\ldots & x_{n m}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "y la variable dependiente es $\\mathrm{Y}$ y tiene solo valor binario, es decir, $0$ o $1$.\n",
    "\n",
    "$$ Y= \\begin{cases}0 & \\text { if Class } 1 \\\\ 1 & \\text { if Class } 2\\end{cases}$$\n",
    "\n",
    "luego, aplique la función multilineal a las variables de entrada $\\mathrm{X}$.\n",
    "\n",
    "$$z=\\left(\\sum_{i=1}^n w_i x_i\\right)+b$$\n",
    "\n",
    "Aquí $x_i$ está la $i$-ésima observación de $\\mathrm{X}, w_i=\\left[w_1, w_2, w_3, \\cdots, w_m\\right]$ son los pesos o coeficiente, y $b$ es el término de sesgo, también conocido como intersección. simplemente esto se puede representar como el producto escalar del peso y el sesgo.\n",
    "\n",
    "$$z=w \\cdot X+b$$\n",
    "\n",
    "La discusión anterior es conocida como... *Regresión Lineal*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663833e5",
   "metadata": {},
   "source": [
    "#### Evaluación del desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363354c",
   "metadata": {},
   "source": [
    "Las métricas de desempeño son esenciales para evaluar y comprender la efectividad de los modelos de clasificación en aprendizaje automático. Cada métrica proporciona una perspectiva única sobre la capacidad del modelo para hacer predicciones correctas y manejar diferentes tipos de errores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd96055",
   "metadata": {},
   "source": [
    "##### Matriz de Confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc34b5",
   "metadata": {},
   "source": [
    "La matriz de confusión es una tabla que muestra las clasificaciones correctas e incorrectas de un modelo, distribuidas en dos dimensiones: las predicciones del modelo y las clases reales. Es especialmente útil para obtener una visión rápida de cómo un modelo está clasificando los diferentes tipos de casos.\n",
    "\n",
    "Los componentes básicos de la matriz de confusión son:  \n",
    "\n",
    "- **Verdaderos Positivos (TP)**: Estos son los casos en los que el modelo predijo correctamente la clase positiva. Por ejemplo, si el modelo está identificando spam, serían los correos spam que el modelo identificó correctamente como spam.  \n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Falsos Positivos (FP)**: Son los casos en los que el modelo predijo incorrectamente la clase positiva. Siguiendo con el ejemplo del spam, serían los correos que no son spam, pero que el modelo identificó erróneamente como tal.  \n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Verdaderos Negativos (TN)**: Son los casos en los que el modelo predijo correctamente la clase negativa. En el caso del spam, serían los correos que no son spam y que el modelo identificó acertadamente como no spam.  \n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Falsos Negativos (FN)**: Son los casos en los que el modelo no pudo identificar correctamente la clase positiva. Estos serían los correos que son spam, pero que el modelo no reconoció como tal.  \n",
    "\n",
    "Se pueden agrupar en una tabla de la siguiente manera:\n",
    "\n",
    "|                   | Predicción Positiva | Predicción Negativa |\n",
    "|-------------------|---------------------|---------------------|\n",
    "| **Clase Positiva** | Verdaderos Positivos (TP) | Falsos Negativos (FN) |\n",
    "| **Clase Negativa** | Falsos Positivos (FP) | Verdaderos Negativos (TN) |\n",
    "\n",
    "\n",
    "***Conexión con las métricas de desempeño***\n",
    "- **Exactitud**: Se calcula directamente de la matriz de confusión como la suma de TP y TN dividida por el total de casos.  \n",
    "<p>&nbsp;</p>\n",
    "   \n",
    "- **Precisión y Sensibilidad (Recall)**: La precisión se calcula utilizando TP y FP, mientras que la sensibilidad se calcula con TP y FN, todos extraídos de la matriz.  \n",
    "<p>&nbsp;</p>\n",
    "   \n",
    "- **Puntuación F1**: Esta puntuación se basa en la precisión y la sensibilidad, que, a su vez, dependen de los valores de la matriz de confusión.  \n",
    "<p>&nbsp;</p>\n",
    "   \n",
    "- La matriz no se utiliza directamente en el cálculo del AUC de la curva ROC, pero proporciona una base para entender cómo las modificaciones en los umbrales de decisión del modelo afectarían a TP, FP, TN y FN.\n",
    "\n",
    "***Interpretación Más Allá de las Métricas:*** Además de facilitar el cálculo de estas métricas, la matriz de confusión permite una interpretación más rica y detallada del rendimiento del modelo. Por ejemplo, en un contexto donde los falsos negativos son muy costosos (como en la detección de enfermedades graves), una matriz de confusión con un alto número de FN señalaría una necesidad crítica de mejora, incluso si la exactitud general del modelo es alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45362efe",
   "metadata": {},
   "source": [
    "##### Exactitud (Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea463e",
   "metadata": {},
   "source": [
    "La exactitud es frecuentemente la primera métrica que se considera en la clasificación. Aunque es intuitiva, su eficacia es limitada, especialmente en conjuntos de datos desbalanceados. Por ejemplo, en una situación donde el 95% de los ejemplos son de una clase, un modelo que siempre predice esa clase alcanzaría una exactitud del 95%, lo cual sería engañoso. Mejor utilizada cuando las clases están equilibradamente distribuidas y los costos de los errores de clasificación son similares entre clases.\n",
    "\n",
    "La exactitud es como el \"promedio general\" en el reporte de calificaciones de un modelo de clasificación. Es una métrica inicial, proporcionando una vista panorámica del rendimiento del modelo. Sin embargo, su eficacia es limitada en situaciones de clases desequilibradas - piensa en un modelo que predice el clima en un desierto y siempre dice 'soleado'. Aunque tendrá una alta exactitud, no es muy útil para los días raros pero críticos cuando llueve.\n",
    "\n",
    "Su representación matemática es:\n",
    "\n",
    "$$ \\text{Exactitud} = \\frac{\\text{Número de predicciones correctas (TP + TN)}}{\\text{Total de predicciones (TP + FP + FN + TN)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0054f4",
   "metadata": {},
   "source": [
    "##### Precisión y Sensibilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e2fd2",
   "metadata": {},
   "source": [
    "La precisión y la sensibilidad nos ofrecen una visión más matizada:\n",
    "\n",
    "- **Precisión**: Indica cuántas de las predicciones clasificadas como positivas son realmente positivas. Es como verificar la calidad de un filtro; por ejemplo, en un filtro de spam, queremos saber: \"De todos los correos marcados como spam, ¿cuántos realmente eran spam?\" \n",
    "\n",
    "  $$ \\text{Precisión} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$  \n",
    "\n",
    "- **Sensibilidad (Recall)**: Muestra cuántos de los casos positivos reales el modelo es capaz de captar. En el mismo contexto de un filtro de spam, sería: \"De todos los correos que son realmente spam, ¿cuántos hemos identificado?\"  \n",
    "\n",
    "  $$ \\text{Sensibilidad} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
    "\n",
    "Estas dos métricas son particularmente valiosas cuando los costos de los falsos positivos y falsos negativos son diferentes. Por ejemplo, en un sistema de diagnóstico médico, un falso negativo (no detectar una enfermedad real) puede ser mucho más grave que un falso positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b25041",
   "metadata": {},
   "source": [
    "##### Puntuación F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e0d9f",
   "metadata": {},
   "source": [
    "La puntuación F1 es como un mediador entre precisión y sensibilidad. No basta con ser bueno en uno y malo en el otro; el F1 Score busca un equilibrio. Su cálculo, al ser el promedio armónico, tiende a penalizar más los desequilibrios extremos entre precisión y sensibilidad.\n",
    "\n",
    "$$ \\text{F1} = 2 \\times \\frac{\\text{Precisión} \\times \\text{Sensibilidad}}{\\text{Precisión} + \\text{Sensibilidad}} $$\n",
    "\n",
    "Este equilibrio es vital en muchos sistemas de clasificación donde tanto evitar falsos positivos como detectar todos los positivos son importantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb18e36",
   "metadata": {},
   "source": [
    "##### Curva ROC y AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3346a0",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/EstructuraDatosAlgoritmos2/blob/main/images/AUC.webp?raw=true\" width=\"250\" />\n",
    "</p>\n",
    "\n",
    "La Curva ROC y el AUC proporcionan una visión integral de cómo se comporta el modelo a través de diferentes umbrales, permitiéndonos entender su capacidad para distinguir entre clases. La Curva ROC es un gráfico de la sensibilidad contra 1 - especificidad (tasa de falsos positivos) para diferentes puntos de corte.\n",
    "\n",
    "El AUC (Area Under the Curve), es como medir el área total bajo una curva en cálculo. Un AUC de 1 significa un modelo perfecto; un AUC de 0.5, no mejor que el azar. Es como darle a tu modelo una \"prueba de estrés\" para ver cómo se comporta en diferentes niveles de decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f2c1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d287d37b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eda727db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78a5b9b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd31e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model accuracy(in %): 96.52294853963839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "\n",
    "# load the digit dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# defining feature matrix(X) and response vector(y)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# splitting X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4,random_state=1)\n",
    "\n",
    "# create logistic regression object\n",
    "reg = linear_model.LogisticRegression()\n",
    "\n",
    "# train the model using the training sets\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# making predictions on the testing set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# comparing actual response values (y_test)\n",
    "# with predicted response values (y_pred)\n",
    "print(\"Logistic Regression model accuracy(in %):\",metrics.accuracy_score(y_test, y_pred)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956892c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c50c74e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "733fe3fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1e36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46cb0456",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Clasificación Binaria:** Este tipo implica solo dos clases o categorías. El objetivo es clasificar las instancias en una de estas dos categorías. Como ejemplos tenemos: Detección de spam (spam o no spam), diagnóstico médico (enfermedad o no enfermedad).\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Clasificación Multiclase (Multinomial):** La clasificación multiclase trata con problemas donde hay más de dos clases, y cada instancia se clasifica en una y solo una de estas clases. Ejemplos: Clasificación de tipos de frutas (manzana, naranja, plátano), reconocimiento de dígitos escritos a mano.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Clasificación Multi-Etiqueta:** En la clasificación multi-etiqueta, cada instancia puede pertenecer a múltiples clases simultáneamente. Tenemos, Categorización de temas en un artículo (política, economía, educación), identificación de varios objetos en una imagen.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Clasificación Jerárquica:** Este tipo organiza las clases en una jerarquía. Las predicciones se hacen a diferentes niveles de esta jerarquía.  Clasificación taxonómica en biología, organización de temas en un sistema de ayuda.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- **Clasificación Desbalanceada:** Se refiere a situaciones en las que las clases no están representadas equitativamente en los datos. Detección de transacciones fraudulentas (pocas transacciones fraudulentas comparadas con las legítimas).\n",
    "\n",
    "Cada uno de estos tipos de clasificación requiere un enfoque y técnicas específicas para abordar de manera efectiva sus desafíos únicos. Por ejemplo, en la clasificación desbalanceada, se pueden emplear métodos para equilibrar los conjuntos de datos o ajustar la sensibilidad del algoritmo hacia la clase minoritaria. En la clasificación multi-etiqueta, se requieren estrategias para manejar múltiples etiquetas simultáneamente. La elección del tipo de clasificación adecuado es crucial para el diseño de un modelo de aprendizaje automático eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36acf3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ecc12ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ec30a0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705875e-1e1c-4e1f-8737-de494aa82135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
